{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d20482",
   "metadata": {},
   "source": [
    "# Bayesian inference of an energy signature model with Rstan\n",
    "\n",
    "Some systems are dependent on a variable, but only above or below a certain value. For example, cooling energy use may be proportional to ambient temperature, yet only above a certain threshold. When ambient temperature decreases to below the threshold, the cooling energy use does not continue to decrease, because the fan energy remains constant. In cases like these, simple regression can be improved by using a change-point linear regression. Change point models often have a better fit than a simple regression, especially when modeling energy usage for a facility.\n",
    "\n",
    "The energy signature of a building decomposes the total energy consumption (or power $\\Phi$) into three terms: heating, cooling, and other uses. Heating and cooling are then assumed to be linearly dependent on the outdoor air temperature $T_e$, and only turned on conditionally on two threshold temperatures $T_{b1}$ and $T_{b2}$, respectively.\n",
    "\n",
    "\\begin{align}\n",
    "  E(\\Phi|\\theta, X) & = \\Phi_0 + \\mathrm{HTC}_1 \\, \\left(T_{b1} - T_e\\right) & \\mathrm{if} \\quad T_e \\leq T_{b1} \\\\\n",
    "  E(\\Phi|\\theta, X) & = \\Phi_0 & \\mathrm{if} \\quad T_{b1} \\leq T_e \\leq T_{b2} \\\\\n",
    "\tE(\\Phi|\\theta, X) & = \\Phi_0 + \\mathrm{HTC}_2 \\, \\left(T_e - T_{b2}\\right) & \\mathrm{if} \\quad T_{b2} \\leq T_e\n",
    "\\end{align}\n",
    "\n",
    "Data points should be averaged over long enough (at least daily) sampling times, so that the steady-state assumption formulated above can hold. $\\Phi_0$ is the average baseline consumption during each sampling period, of all energy uses besides heating and cooling. Heating is turned on if the outdoor air temperature drops below a basis temperature $T_{b1}$, and the heating power $\\Phi_h = \\mathrm{HTC}_1 \\, \\left(T_{b1} - T_e\\right)$ is assumed proportional to a heat transfer coefficeint (HTC) value. The same reasoning is used to formulate cooling, with a \"summer HTC\" value that may be different from the first one. This model is therefore a piecewise linear regression model, where the switching points $T_{b1}$ and $T_{b2}$ are usually to be identified along with the other parameters.\n",
    "\n",
    "The appeal of the energy signature model is that the only data it requires are energy meter readings and outdoor air temperature, with a large sampling time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e66eaa",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "library(rstan)\n",
    "library(tidyverse)\n",
    "library(lubridate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff3bd5",
   "metadata": {},
   "source": [
    "The data used in this example is the hourly energy consumption and outdoor air temperature data for 11 commercial buildings (office/retail), publicly available here:\n",
    "\n",
    "https://openei.org/datasets/dataset/consumption-outdoor-air-temperature-11-commercial-buildings\n",
    "\n",
    "We will be using two data files, respectively labeled *Building 6 (Office \"Pre\")*, and *Building 6 (Office \"Post\")*.\n",
    "\n",
    "## Loading and displaying the data\n",
    "\n",
    "The following block loads two separate data files:\n",
    "\n",
    "* `building60preoffice.csv` is the baseline period file, saved into the df.base variable\n",
    "* `building60postoffice.csv` is the reporting period file, saved into the df.repo variable\n",
    "\n",
    "The `Date` column of both files is converted into a DateTime type into a new column. Then, the baseline dataset is displayed for a first exploratory look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f011e",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "# Baseline data: one year\n",
    "df.base <- read_csv(\"data/building60preoffice.csv\") %>% \n",
    "  mutate(DateTime = mdy_hm(Date),\n",
    "         Date = as_date(DateTime))\n",
    "\n",
    "# Post-retrofit data: one year\n",
    "df.repo <- read_csv(\"data/building61duringoffice.csv\") %>% \n",
    "  mutate(DateTime = mdy_hm(Date),\n",
    "         Date = as_date(DateTime))\n",
    "\n",
    "# Plot the original data\n",
    "head(df.base)\n",
    "ggplot(data = df.base) + geom_line(mapping = aes(x=DateTime, y=`Building 6 kW`))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb30d11",
   "metadata": {},
   "source": [
    "A few interesting observations:\n",
    "\n",
    "* The data has a hourly time step size. Every hour, the outdoor air temperature (OAT in °F) and energy use (kW) are available.\n",
    "* The energy use is higher in summer and in winter than in-between. This suggests that this consumption data includes both heating and cooling appliances.\n",
    "* Week-ends are clearly visible with a lower consumption than in the working days of the week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872083e7",
   "metadata": {},
   "source": [
    "## Daily averaged data\n",
    "\n",
    "Averaging the data over daily time steps should allow to overlook the dependence between consecutive measurements. In turn, this allows using a model which will be much simpler than time series models, but will only be capable of low frequency predictions.\n",
    "\n",
    "The following block creates new datasets from the original ones:\n",
    "\n",
    "* Measurements are daily averaged\n",
    "* Temperatures are switched to °C for international suitability.\n",
    "* A categorical variable is added to indicate week ends.\n",
    "\n",
    "Then, we plot the daily energy use $E$ (kWh) versus the outdoor temperature $T$ (°C) for both values of the `week.end` categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5c18d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "daily.average <- function(df) {\n",
    "  df %>% \n",
    "    group_by(Date) %>% \n",
    "    summarise(OAT = mean(OAT),\n",
    "              E = sum(`Building 6 kW`),\n",
    "              .groups = 'drop'\n",
    "    ) %>% \n",
    "    mutate(wday = wday(Date),\n",
    "           week.end = wday==1 | wday==7,\n",
    "           T = (OAT-32) * 5/9)\n",
    "}\n",
    "\n",
    "df.base.daily <- daily.average(df.base)\n",
    "df.repo.daily <- daily.average(df.repo)\n",
    "\n",
    "ggplot(data = df.base.daily) +\n",
    "  geom_point(mapping = aes(x=T, y=E, color=week.end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537cea91",
   "metadata": {},
   "source": [
    "# Modelling and training\n",
    "\n",
    "## Step 1: Model definition\n",
    "\n",
    "After looking at the data, we can suggest using a change-point model which will include the effects of heating and cooling, and separate week ends from working days. The expected daily energy use $E$ (in kWh per day) is a function of the outdoor temperature $T$ and of a number of parameters:\n",
    "\n",
    "* For the week-ends: $p(E|T) \\sim \\mathcal{N}\\left[\\alpha_1 + \\beta_{1,h}(\\tau_{1,h}-T)^+ + \\beta_{1,c}(T-\\tau_{1,c})^+, \\sigma\\right]$\n",
    "* For the working days: $p(E|T) \\sim \\mathcal{N}\\left[\\alpha_2 + \\beta_{2,h}(\\tau_{2,h}-T)^+ + \\beta_{2,c}(T-\\tau_{2,c})^+,\\sigma\\right]$\n",
    "\n",
    "Where the 1 and 2 subscripts indicate week-ends and working day, respectively, and the $h$ and $c$ subscripts indicate heating and cooling modes. The $+$ superscript indicates that a term is only applied if above zero.\n",
    "\n",
    "The two equations above mean that we expect the energy use $E$ to be a normal distribution centered around a change-point model, with a constant standard deviation $\\sigma$. Some particularities of Bayesian statistics are: this normal distribution can be replaced by any other probability distribution; the error term $\\sigma$ can be formulated as a function of some inputs; etc.\n",
    "\n",
    "**This model has 11 possible parameters**, which makes it significantly more complex than an ordinary linear regression. We could simplify it by assuming that the \"working days\" and \"week ends\" mode share the same temperature thresholds for heating ($\\tau_{1,h}=\\tau_{2,h}$) or for cooling ($\\tau_{1,c}=\\tau_{2,c}$). The following method would also be exactly the same if we decided to complexify the model, for instance by assuming non-linear profiles on each side of the change points, or if we had more categorical variables.\n",
    "\n",
    "## Model specification with STAN\n",
    "\n",
    "In this example, we use the STAN probabilistic programming language, which allows full Bayesian statistical inference.\n",
    "\n",
    "https://mc-stan.org/\n",
    "\n",
    "A STAN model is a block of text which can either be written in a separate file, or in the same script as the current code. Specifying a model in STAN takes a certain learning curve, but it unlocks the full flexibility of Bayesian analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed715b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "changepoint <- \"\n",
    "functions {\n",
    "  // This chunk is the formula for the changepoint model which will be used several times in this program\n",
    "  real power_mean(int w, real t, vector alpha, vector beta_h, vector tau_h, vector beta_c, vector tau_c) {\n",
    "    real a = w ? alpha[1] : alpha[2];    // condition on the type of day\n",
    "    real heat = w ? beta_h[1] * fmax(tau_h[1]-t, 0) : beta_h[2] * fmax(tau_h[2]-t, 0) ;\n",
    "    real cool = w ? beta_c[1] * fmax(t-tau_c[1], 0) : beta_c[2] * fmax(t-tau_c[2], 0) ;\n",
    "    return (a + heat + cool);\n",
    "  }\n",
    "}\n",
    "data {\n",
    "  // This block declares all data which will be passed to the Stan model.\n",
    "  int<lower=0> N_base;        // number of data items in the baseline period\n",
    "  vector[N_base] t_base;      // temperature (baseline)\n",
    "  int w_base[N_base];      // categorical variable for the week ends (baseline)\n",
    "  vector[N_base] y_base;      // outcome energy vector (baseline)\n",
    "  \n",
    "  int<lower=0> N_repo;        // number of data items in the reporting period\n",
    "  vector[N_repo] t_repo;      // temperature (reporting)\n",
    "  int w_repo[N_repo];      // categorical variable for the week ends (reporting)\n",
    "  vector[N_repo] y_repo;      // outcome energy vector (reporting)\n",
    "}\n",
    "parameters {\n",
    "  // This block declares the parameters of the model. There are 10 parameters plus the error scale sigma\n",
    "  vector[2] alpha;      // baseline consumption (work days and week ends)\n",
    "  vector[2] beta_h;     // slopes for heating\n",
    "  vector[2] tau_h;      // threshold temperatures for heating\n",
    "  vector[2] beta_c;     // slopes for cooling\n",
    "  vector[2] tau_c;      // threshold temperatures for cooling\n",
    "  real<lower=0> sigma;  // error scale\n",
    "}\n",
    "model {\n",
    "  // Assigning prior distributions on some parameters\n",
    "  alpha ~ normal([400, 800], [150, 150]);\n",
    "  tau_h ~ normal(8, 5);\n",
    "  tau_c ~ normal(18, 5);\n",
    "  beta_h ~ normal(40, 15);\n",
    "  beta_c ~ normal(40, 15);\n",
    "  // Observational model\n",
    "  for (n in 1:N_base) {\n",
    "    y_base[n] ~ normal(power_mean(w_base[n], t_base[n], alpha, beta_h, tau_h, beta_c, tau_c), sigma);\n",
    "  }\n",
    "}\n",
    "generated quantities {\n",
    "  vector[N_base] y_base_pred;\n",
    "  vector[N_repo] y_repo_pred;\n",
    "  real savings = 0;\n",
    "  \n",
    "  for (n in 1:N_base) {\n",
    "    y_base_pred[n] = normal_rng(power_mean(w_base[n], t_base[n], alpha, beta_h, tau_h, beta_c, tau_c), sigma);\n",
    "  }\n",
    "  \n",
    "  for (n in 1:N_repo) {\n",
    "    y_repo_pred[n] = normal_rng(power_mean(w_repo[n], t_repo[n], alpha, beta_h, tau_h, beta_c, tau_c), sigma);\n",
    "    savings += y_repo_pred[n] - y_repo[n];\n",
    "  }\n",
    "}\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0d731",
   "metadata": {},
   "source": [
    "Then, a list called `model_data` is created, which maps each part of the data to its appropriate variable into the STAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16bd07",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model_data <- list(\n",
    "  N_base = nrow(df.base.daily),\n",
    "  t_base = df.base.daily$T,\n",
    "  w_base = as.numeric(df.base.daily$week.end),\n",
    "  y_base = df.base.daily$E,\n",
    "  N_repo = nrow(df.repo.daily),\n",
    "  t_repo = df.repo.daily$T,\n",
    "  w_repo = as.numeric(df.repo.daily$week.end),\n",
    "  y_repo = df.repo.daily$E\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9747ad",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "Now that the model has been specified and the data has been mapped to its variables, the syntax for model fitting is below.\n",
    "\n",
    "One disadvantage of Bayesian inference is that the MCMC algorithm takes much longer to converge than a typical least-squares model fitting method. Running the code below might take a minute because we are only using 365 data points, but the Bayesian approach might become problematic for larger data files (100,000 rows or more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d97d6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "fit1 <- stan(\n",
    "  model_code = changepoint,  # Stan program\n",
    "  data = model_data,        # named list of data\n",
    "  chains = 2,               # number of Markov chains\n",
    "  warmup = 1000,            # number of warmup iterations per chain\n",
    "  iter = 4000,              # total number of iterations per chain\n",
    "  cores = 2,                # number of cores (could use one per chain)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21221121",
   "metadata": {},
   "source": [
    "Fitting may result in a number of warnings, telling us that some problems may have occurred: divergent transitions, large R-hat values, low Effective Sample Size... Obtaining a fit without these warnings takes some practice but is essential for an unbiased interpretation of the inferred variables and predictions. A guide to Stan's warnings and how to address them is available here: https://mc-stan.org/misc/warnings.html\n",
    "\n",
    "The first step into solving these warnings is to re-run the algorithm with different controls: `iter`, `max_treedepth`, etc. If problems persist, it is possible that the model is too complex for the information that the data is able to provide and should be simplified, or that stronger priors should be proposed. A lot of problems can be solved with some prior information. In our specific case, this is especially useful for the variables in the equation for the week-ends, since there are not a lot of data points.\n",
    "\n",
    "# Validation and results\n",
    "\n",
    "Stan returns an object (called `fit1` above) from which the distributions of outputs and parameters of the fitted model can be accessed\n",
    "\n",
    "https://cran.r-project.org/web/packages/rstan/vignettes/stanfit-objects.html\n",
    "\n",
    "The MCMC algorithm produces a chain of samples $\\theta^{(m)}$ for the parameters, which approximate their posterior distributions. In this case, each parameter of the model is represented by a chain of 6,000 draws: from these draws, we can extract any statistics we need: mean, median, quantiles, $t$-score and $p$-values, etc.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "As a first validation step, it is useful to take a look at the values of the parameters that have been estimated by the algorithm. Below, we use three diagnostics tools:\n",
    "\n",
    "* The `print` method shows the table of parameters, much like we could display after an ordinary linear regression\n",
    "* `traceplot` shows the traces of the selected parameters. If the fitting has converged, the traces approximate the posterior distributions\n",
    "* `pairs` shows the pairwise relationships between parameters. Strong interactions between some parameters are an indication that the model should be re-parameterised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit1, pars = c(\"alpha\", \"beta_h\", \"tau_h\", \"beta_c\", \"tau_c\", \"sigma\", \"savings\"))\n",
    "traceplot(fit1, pars = c(\"alpha\", \"beta_h\", \"tau_h\", \"beta_c\", \"tau_c\", \"sigma\", \"lp__\"))\n",
    "pairs(fit1, pars = c(\"alpha\", \"beta_h\", \"savings\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ffa65",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "https://mc-stan.org/docs/2_26/stan-users-guide/posterior-prediction-chapter.html\n",
    "\n",
    "Our main goal here is to compare the energy use measured during the reporting period $y_\\mathit{repo}$ with the predictions of the fitted model. Since it is a probabilistic model, its outcome is actually a probability distribution $p\\left(y_\\mathit{repo}|x_\\mathit{repo}, x_\\mathit{base}, y_\\mathit{base}\\right)$, based on the observed values of the model inputs $x$ during the baseline and reporting periods, and on the observed energy use during the baseline period $y_\\mathit{base}$.\n",
    "\n",
    "This so-called **posterior predictive distribution $p\\left(y_\\mathit{repo}|...\\right)$** is already directly available, because a value of $y_\\mathit{repo}$ (for each time step) was directly calculated by the Stan model for each value $\\theta^{(m)}$.\n",
    "\n",
    "$$ p\\left(y_\\mathit{repo}|...\\right) \\approx \\frac{1}{M} \\sum_{m=1}^M p\\left(y_\\mathit{repo}|x_\\mathit{repo},\\theta^{(m)}\\right) $$\n",
    "First, let us look at the posterior predictive distribution during the baseline period, in order to validate the model compared to its training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fba94",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Extracting full predictive distributions from the stanfit object\n",
    "la <- rstan::extract(fit1, permuted = TRUE)\n",
    "y_base_pred <- la$y_base_pred\n",
    "\n",
    "# Quantiles\n",
    "y_base_quan <- apply(y_base_pred, 2, quantile, probs=c(0.025, 0.5, 0.975))\n",
    "\n",
    "# Data frame\n",
    "df.base.post <- data.frame(Date = df.base.daily$Date, T = df.base.daily$T, y = df.base.daily$E, w = df.base.daily$week.end,\n",
    "                           pred_low = y_base_quan[1, ], pred_med = y_base_quan[2, ], pred_up = y_base_quan[3, ])\n",
    "\n",
    "# Plot\n",
    "ggplot(data = df.base.post) +\n",
    "  geom_point(mapping = aes(x=T, y=y, color=w)) +\n",
    "  geom_line(data = . %>% filter(!df.base.post$w), mapping = aes(x=T, y=pred_med), color='red') +\n",
    "  geom_ribbon(data = . %>% filter(!df.base.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill='red', alpha=0.1) +\n",
    "  geom_line(data = . %>% filter(df.base.post$w), mapping = aes(x=T, y=pred_med), color='blue') +\n",
    "  geom_ribbon(data = . %>% filter(df.base.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill='blue', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0aaeda",
   "metadata": {},
   "source": [
    "The colored bands show a 95% prediction interval for the working days and the week ends, respectively. The points are the measurements of the baseline period.\n",
    "\n",
    "## Residuals\n",
    "\n",
    "An important validation step is to check for autocorrelation in the residuals of the fitted model, on the baseline data that was used for fitting. Autocorrelation is often a sign of insufficient model complexity, or that the form of the model error term has not been appropriately chosen.\n",
    "\n",
    "The two graphs below show:\n",
    "\n",
    "* Residuals vs Date, in order to display eventual autocorrelation\n",
    "* residuals vs Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b57bd69",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ggplot(data = df.base.post) +\n",
    "  geom_point(mapping = aes(x=Date, y=pred_med-y)) +\n",
    "  geom_ribbon(mapping = aes(x=Date, ymin=pred_low-y, ymax=pred_up-y), alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cf35c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ggplot(data = df.base.post) +\n",
    "  geom_point(mapping = aes(x=T, y=pred_med-y)) +\n",
    "  geom_ribbon(mapping = aes(x=T, ymin=pred_low-y, ymax=pred_up-y), alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118aeeb2",
   "metadata": {},
   "source": [
    "The second graph is fine, but it seems that these is a trend in the residuals in the first few months and last few months of the year, suggesting that the model doesn't quite capture the winter energy consumption very well.\n",
    "\n",
    "# Savings\n",
    "\n",
    "Our Stan model already calculates the expected output $y$ of the reporting period, for each sample $\\theta_i$ of the posterior distribution. We can therefore display a probability distribution for each of the data points of the reporting period, and compare it with the measured data in the same period.\n",
    "\n",
    "The following graph compares the energy use measured during the **reporting period** (points) with the probability distributions of energy use predicted by the model during the same period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99199938",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Extracting full predictive distributions from the stanfit object\n",
    "y_repo_pred <- la$y_repo_pred\n",
    "# Quantiles\n",
    "y_repo_quan <- apply(y_repo_pred, 2, quantile, probs=c(0.025, 0.5, 0.975))\n",
    "# Data frame\n",
    "df.repo.post <- data.frame(Date = df.repo.daily$Date, T = df.repo.daily$T, y = df.repo.daily$E, w = df.repo.daily$week.end,\n",
    "                           pred_low = y_repo_quan[1, ], pred_med = y_repo_quan[2, ], pred_up = y_repo_quan[3, ])\n",
    "# Plot\n",
    "ggplot(data = df.repo.post) +\n",
    "  geom_point(mapping = aes(x=T, y=y, color=w)) +\n",
    "  geom_line(data = . %>% filter(!df.repo.post$w), mapping = aes(x=T, y=pred_med), color='red') +\n",
    "  geom_ribbon(data = . %>% filter(!df.repo.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill='red', alpha=0.1) +\n",
    "  geom_line(data = . %>% filter(df.repo.post$w), mapping = aes(x=T, y=pred_med), color='blue') +\n",
    "  geom_ribbon(data = . %>% filter(df.repo.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill='blue', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb7b7b9",
   "metadata": {},
   "source": [
    "The savings, i.e. the difference between the measured energy use during the reporting period and their prediction by the model, have been included in the Stan model definition. Similarly to the prediction, **the savings are therefore available as a probability distribution**: we have a full description of any confidence interval we may wish for.\n",
    "\n",
    "The table of results shown after model fitting shows that\n",
    "\n",
    "* The mean estimated savings are 69,069 kWh\n",
    "* The 95% confidence interval spans between 63,550 and 74,880 kWh\n",
    "\n",
    "We can also choose to display any quantile of the posterior distribution of savings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ada035",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot(fit1, pars = c(\"savings\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f8ddce",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "warning,message,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
